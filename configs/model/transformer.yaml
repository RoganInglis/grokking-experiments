_target_: src.models.transformer_module.TransformerLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001
  betas: [0.9, 0.98]
  weight_decay: 1.0

#scheduler:
#  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#  _partial_: true
#  mode: min
#  factor: 0.1
#  patience: 1000000000  # effectively no decay

net:
  _target_: src.models.components.transformers.TransformerWrapper
  vocab_size: 114  # p + 1
  dim_model: 128
  num_heads: 4
  dim_heads: 32
  dim_inner: 512
  num_layers: 1
  dropout: 0.
  tie_emb_weights: false
  max_len: 3
  layer_norm: false

